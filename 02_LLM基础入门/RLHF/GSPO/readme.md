### 🚀 GSPO算法：序列级优化的范式转变 

> **论文标题**: [《Group Sequence Policy Optimization》](https://arxiv.org/pdf/2507.18071)
> **机构**: 阿里巴巴Qwen团队
> **发表时间**: 2025年
> **核心创新**: 从Token级到Sequence级的优化范式革命

## 🎯 算法定位与核心价值 

GSPO(Group Sequence Policy Optimization)是阿里巴巴Qwen团队提出的革命性算法，**将优化的基本单位从"词元"提升到"序列"**，从根本上解决了奖励粒度与优化目标不匹配的问题，特别适合处理长文本和MoE架构模型。

### ⚡ 技术突破亮点 

- **优化粒度**: Token级 → Sequence级
- **训练稳定性**: 显著降低优化方差
- **MoE适配性**: 专门针对混合专家模型优化
- **计算效率**: 训练时间减少30-40%

## 🏗️ 核心技术架构 

### 🔄 序列级优势估计 

GSPO采用基于组的优势估计机制，从根本上改变了传统RLHF的优化范式：序列级裁剪避免了传统token级裁剪的不稳定性，使模型在训练过程中保持更平滑的性能提升。

### 🧮 损失函数设计 

**GSPO损失函数**采用序列级优化目标：**核心创新**在于将重要性采样和优化的粒度提升到序列级别，使优化目标与奖励授予单位一致，从根本上解决了GRPO中的不稳定因素。

## 📊 性能优势验证 

### 🚀 训练稳定性突破 

GSPO在训练稳定性方面展现显著优势：

- **GRPO问题**: 训练曲线频繁出现剧烈抖动(模型崩溃)
- **GSPO优势**: 曲线呈现平滑上升趋势(稳定优化)
- **效率提升**: 相同计算资源下，达到同等性能所需训练时间减少30-40%

### 🏆 MoE模型专项优化 

GSPO特别适合MoE模型的训练：

- **整体质量关注**: 关注序列整体生成质量而非单个token路径
- **路由简化**: 摆脱对专家激活路径的依赖，消除复杂"路由重放"需求
- **实践效果**: 在Qwen3-30B MoE模型训练中，无需额外稳定性策略即可收敛，训练效率提升2倍以上

## 🔄 GSPO-token变体：灵活性与稳定性的平衡 

### 🎯 细粒度优化需求 

在多轮对话强化学习等场景中，需要对优势函数进行比序列级别更细粒度的调整。为此引入**GSPO-token**变体：

### 💡 技术特点 

**GSPO-token**的创新特性：

- **平等调整**: 根据整个响应质量平等调整每个词语的参数
- **灵活优势**: 允许为不同位置token设置不同的优势值
- **向后兼容**: 当所有token优势值相同时，与GSPO数值等价
- **未来支持**: 为更细粒度(token级别)的优势调整提供基础架构

## 📈 算法对比分析 

### 🆚 RLHF算法全家福对比 

| 算法         | 优势函数计算           | Clip处理                  | Loss设计                  | 主要改进点             | 适用场景               |
| ------------ | ---------------------- | ------------------------- | ------------------------- | ---------------------- | ---------------------- |
| 🟥 PPO        | GAE(全局)              | 对称裁剪([1-ε, 1+ε])      | Token级梯度+KL惩罚+熵正则 | 稳定性与探索的平衡     | 通用RL任务             |
| 🟦 DPO        | 直接偏好对齐           | 无裁剪                    | 序列级偏好对齐+熵正则     | 绕过奖励模型训练       | 复杂推理任务           |
| 🟩 GRPO       | 组内标准化             | 对称裁剪                  | Token级梯度+KL惩罚        | 组内相对比较替代Critic | 人类偏好对齐           |
| 🟪 Dr.GRPO    | 组内均值               | 对称裁剪                  | Token级梯度+KL惩罚        | 移除std和长度归一化    | 长文本推理任务         |
| 🟨 DAPO       | 组内标准化             | 非对称裁剪([1-ε], [1+ε']) | Token级梯度+软惩罚        | Clip-Higher和动态采样  | 通用LLM对齐            |
| 🟧 GSPO       | 序列级优势             | 序列级裁剪                | 序列级梯度                | 序列级优化替代token级  | MoE模型和长文本任务    |
| 🟨 GSPO-token | 序列级权重+token级优势 | 序列级裁剪                | Token级梯度+序列级权重    | 灵活性与稳定性的平衡   | 多轮对话等局部优化任务 |

### 💡 技术演进脉络 

```Plain
PPO (2017) → DPO (2023) → GRPO (2024) → [Dr.GRPO → DAPO → GSPO] (2024-2025)
```

*从稳定性基础到效率优化，再到序列级**范式**的完整技术链*

## 🎯 核心技术创新 

### 🔄 范式转变的意义 

1. **粒度对齐**: 优化粒度与奖励粒度完美匹配
2. **方差降低**: 序列级优化显著降低梯度方差
3. **稳定性提升**: 避免token级优化的局部震荡
4. **效率优化**: 减少不必要的细粒度计算

### 🏗️ 架构优势 

```Plain
class GSPOOptimizer:
    def __init__(self, sequence_level=True):
        self.sequence_level = sequence_level
        
    def compute_gradient(self, sequences, rewards):
        if self.sequence_level:
            # 序列级梯度计算
            return self.sequence_gradient(sequences, rewards)
        else:
            # 传统token级梯度计算
            return self.token_gradient(sequences, rewards)
```

## 🚀 应用场景与前景 

### 🎯 当前优势场景 

- **MoE模型训练**: 专门优化的架构适配
- **长文本生成**: 序列级连贯性保持
- **复杂推理**: 多步推理任务优化
- **多轮对话**: 对话序列整体优化

### 🔮 未来发展方向 

1. **多模态扩展**: 视觉-语言序列任务适配
2. **自适应机制**: 动态粒度调整策略
3. **分布式优化**: 大规模序列学习框架
4. **元学习集成**: 智能优化策略选择

## 💎 总结评价 

GSPO代表了RLHF技术发展的重大里程碑，通过**序列级优化范式**的创新，实现了：

> 🎯 **根本性突破**: 优化粒度与奖励粒度的完美对齐⚡ **稳定性革命**: 显著降低训练方差，提升收敛稳定性🔄 **架构创新**: 专门为现代大模型架构(MoE、长上下文)优化🚀 **效率提升**: 训练时间减少30-40%，MoE训练效率提升2倍

GSPO不仅解决了当前RLHF技术的核心痛点，更重要的是为处理更复杂的序列决策问题奠定了技术基础，推动了大模型对齐技术向更高效、更稳定的方向发展。<div align="center">

<span style="color:#666;font-size:0.9em">📄 基于阿里巴巴Qwen团队研究成果整理 · 序列级优化范式的开创性工作</span></div>


**By：猫先生 of 「魔方AI空间」**

![alt text](../../../imgs/魔方AI空间.png)